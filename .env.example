# BIOwerk Configuration
# Copy this file to .env and update with your values

# ============================================================================
# PostgreSQL Configuration
# ============================================================================
# When using PgBouncer (recommended for production):
# - Set POSTGRES_HOST=pgbouncer and POSTGRES_PORT=6432
# When connecting directly to PostgreSQL (development only):
# - Set POSTGRES_HOST=postgres and POSTGRES_PORT=5432
POSTGRES_HOST=pgbouncer
POSTGRES_PORT=6432
POSTGRES_USER=biowerk
POSTGRES_PASSWORD=biowerk_dev_password
POSTGRES_DB=biowerk

# ============================================================================
# PgBouncer Connection Pooling Configuration
# ============================================================================
# Enable PgBouncer for production-grade connection pooling
# Reduces PostgreSQL memory usage and improves connection reuse

# Pool Mode (transaction | session | statement)
# - transaction: Recommended for microservices (best connection reuse)
#   Connection returned to pool after each transaction
# - session: Traditional mode, connection held until client disconnects
# - statement: Aggressive pooling, connection returned after each statement
PGBOUNCER_POOL_MODE=transaction

# Connection Limits
# Maximum client connections (from application services)
# Formula: (num_services * connections_per_service) + overhead
PGBOUNCER_MAX_CLIENT_CONN=200

# Default pool size (server connections to PostgreSQL)
# This is the main pool of connections kept alive
# Start with expected concurrent queries / databases
PGBOUNCER_DEFAULT_POOL_SIZE=25

# Minimum pool size (always maintained)
# Ensures warm connections are available
PGBOUNCER_MIN_POOL_SIZE=10

# Reserve pool (emergency connections)
# Extra connections when default pool exhausted
PGBOUNCER_RESERVE_POOL_SIZE=10

# Maximum database connections (including reserve)
# Should be less than PostgreSQL max_connections (default: 100)
PGBOUNCER_MAX_DB_CONNECTIONS=50

# Timeouts (seconds)
# Server idle timeout - close idle connections
PGBOUNCER_SERVER_IDLE_TIMEOUT=600

# Logging
# Set to 1 to log connections/disconnections, 0 to disable
PGBOUNCER_LOG_CONNECTIONS=1
PGBOUNCER_LOG_DISCONNECTIONS=1

# ============================================================================
# MongoDB Configuration
# ============================================================================
MONGO_HOST=mongodb
MONGO_PORT=27017
MONGO_USER=biowerk
MONGO_PASSWORD=biowerk_dev_password
MONGO_DB=biowerk

# ============================================================================
# Redis Configuration
# ============================================================================
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0

# ============================================================================
# Cache Configuration
# ============================================================================
CACHE_TTL=300
CACHE_ENABLED=true

# ============================================================================
# Application Configuration
# ============================================================================
LOG_LEVEL=INFO
ENVIRONMENT=development

# ============================================================================
# Authentication Configuration
# ============================================================================
# IMPORTANT: Change JWT_SECRET_KEY in production! Use: openssl rand -hex 32
JWT_SECRET_KEY=dev-secret-key-change-in-production-use-openssl-rand-hex-32
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7
API_KEY_HEADER=X-API-Key
REQUIRE_AUTH=false

# ============================================================================
# TLS/HTTPS Configuration
# ============================================================================
# Enable TLS/HTTPS for production (requires valid certificates)
# For development, generate self-signed certs: python scripts/generate_certs.py
TLS_ENABLED=false
TLS_CERT_FILE=./certs/cert.pem
TLS_KEY_FILE=./certs/key.pem
TLS_CA_FILE=
TLS_VERIFY_CLIENT=false
# Minimum TLS version: TLSv1.2 or TLSv1.3 (TLSv1.3 recommended for production)
TLS_MIN_VERSION=TLSv1.2
# Custom cipher suite (leave empty for secure defaults)
TLS_CIPHERS=

# ============================================================================
# Rate Limiting Configuration
# ============================================================================
# Enable rate limiting to prevent abuse and DDoS attacks
RATE_LIMIT_ENABLED=true
# Number of requests allowed per time window
RATE_LIMIT_REQUESTS=100
# Time window in seconds
RATE_LIMIT_WINDOW=60
# Strategy: fixed_window, sliding_window (recommended), or token_bucket
RATE_LIMIT_STRATEGY=sliding_window
# Apply rate limiting per IP address
RATE_LIMIT_PER_IP=true
# Apply rate limiting per authenticated user
RATE_LIMIT_PER_USER=true
# Burst size for token bucket strategy (allows temporary bursts)
RATE_LIMIT_BURST=20
# Paths to exclude from rate limiting (comma-separated)
# RATE_LIMIT_EXCLUDE_PATHS=/health,/metrics

# ============================================================================
# Agent URLs (for mesh gateway)
# ============================================================================
AGENT_OSTEON_URL=http://osteon:8001
AGENT_MYOCYTE_URL=http://myocyte:8002
AGENT_SYNAPSE_URL=http://synapse:8003
AGENT_CIRCADIAN_URL=http://circadian:8004
AGENT_NUCLEUS_URL=http://nucleus:8005
AGENT_CHAPERONE_URL=http://chaperone:8006

# ============================================================================
# Audit Logging Configuration
# ============================================================================
# Enable comprehensive audit logging with encryption at rest
AUDIT_ENABLED=true
# Log all API requests (captures method, path, headers, body)
AUDIT_LOG_REQUESTS=true
# Log all API responses (captures status, headers, body)
AUDIT_LOG_RESPONSES=true
# Encrypt sensitive fields (IP addresses, request/response data)
AUDIT_ENCRYPT_SENSITIVE=true
# Default retention period in days (1 year)
AUDIT_RETENTION_DAYS=365
# Retention for authentication events (90 days)
AUDIT_RETENTION_AUTH_DAYS=90
# Retention for data modification events (7 years for compliance)
AUDIT_RETENTION_DATA_DAYS=2555
# Retention for security events (2 years)
AUDIT_RETENTION_SECURITY_DAYS=730
# Collect geolocation data (requires external GeoIP service)
AUDIT_COLLECT_GEO=false
# Maximum size for request/response fields (64KB)
AUDIT_MAX_FIELD_SIZE=65536
# Batch size for bulk audit log writes
AUDIT_BATCH_SIZE=100
# Write audit logs asynchronously for better performance
AUDIT_ASYNC_WRITE=true

# ============================================================================
# Encryption Configuration (for Audit Logs and Sensitive Data)
# ============================================================================
# Enable encryption at rest for sensitive data
ENCRYPTION_ENABLED=true
# IMPORTANT: Master encryption key - MUST be changed in production!
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
# In production, use KMS (AWS KMS, Azure Key Vault, HashiCorp Vault)
ENCRYPTION_MASTER_KEY=change-this-master-key-in-production-min-32-chars-required
# Current encryption key version (increment when rotating keys)
ENCRYPTION_KEY_VERSION=1
# Days before key rotation is recommended (90 days)
ENCRYPTION_KEY_ROTATION_DAYS=90
# Base64-encoded salt for key derivation (auto-generated if not set)
ENCRYPTION_SALT=
# Encryption algorithm (DO NOT CHANGE unless you know what you're doing)
ENCRYPTION_ALGORITHM=AES-256-GCM

# ============================================================================
# Data Retention Policy Configuration (SOC2, HIPAA, GDPR Compliance)
# ============================================================================
# Enable automated data retention policy enforcement
RETENTION_ENABLED=true

# Automated policy evaluation interval (hours)
# How often to check and enforce retention policies
RETENTION_EVALUATION_INTERVAL_HOURS=24

# Archive cleanup interval (hours)
# How often to delete expired archives
RETENTION_ARCHIVE_CLEANUP_INTERVAL_HOURS=168

# Default retention periods by data type (days)
# These are overridden by specific retention policies
RETENTION_DEFAULT_AUDIT_LOG_DAYS=365
RETENTION_DEFAULT_USER_DATA_DAYS=2555
RETENTION_DEFAULT_EXECUTION_DAYS=90
RETENTION_DEFAULT_PROJECT_DAYS=730
RETENTION_DEFAULT_ARTIFACT_DAYS=730
RETENTION_DEFAULT_API_KEY_DAYS=90
RETENTION_DEFAULT_SESSION_DAYS=30
RETENTION_DEFAULT_CACHE_DAYS=7

# Archive expiration period (days)
# How long to keep archived data before permanent deletion
# Set to 0 to keep archives indefinitely
RETENTION_ARCHIVE_EXPIRATION_DAYS=2555

# Require archival before deletion
# If true, all deletions must be preceded by archival
RETENTION_REQUIRE_ARCHIVE_BEFORE_DELETE=true

# Enable legal hold support
# Allows administrators to prevent data deletion for litigation/investigation
RETENTION_LEGAL_HOLD_ENABLED=true

# Compliance framework notifications
# Send alerts for compliance violations and policy conflicts
RETENTION_COMPLIANCE_ALERTS_ENABLED=true

# Dry run mode (for testing)
# If true, policies are evaluated but no data is actually deleted
RETENTION_DRY_RUN=false

# ============================================================================
# LLM Provider Configuration
# ============================================================================
# Primary LLM provider: 'openai', 'anthropic', 'deepseek', 'ollama', or 'local'
# - 'local': Use standalone GGUF models in service directories (NO git commit)
# - 'ollama': Use shared Ollama server (current default)
# - 'openai', 'anthropic', 'deepseek': Cloud APIs (requires keys)
LLM_PROVIDER=ollama

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4o
OPENAI_MAX_TOKENS=4096
OPENAI_TEMPERATURE=0.7
OPENAI_TIMEOUT=60

# Anthropic Claude Configuration
ANTHROPIC_API_KEY=your-anthropic-api-key-here
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
ANTHROPIC_MAX_TOKENS=4096
ANTHROPIC_TEMPERATURE=0.7
ANTHROPIC_TIMEOUT=60

# DeepSeek Configuration
DEEPSEEK_API_KEY=your-deepseek-api-key-here
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_BASE_URL=https://api.deepseek.com
DEEPSEEK_MAX_TOKENS=4096
DEEPSEEK_TEMPERATURE=0.7
DEEPSEEK_TIMEOUT=60

# Ollama Configuration (Local/Open-Source LLMs)
# Recommended models: phi3:mini, llama3.2, mistral, qwen2.5:7b
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=phi3:mini
OLLAMA_MAX_TOKENS=4096
OLLAMA_TEMPERATURE=0.7
OLLAMA_TIMEOUT=120

# Local Model Configuration (Standalone GGUF files - NOT in git)
# Download models using: ./scripts/download-models.sh
# Each service has its own copy in services/SERVICE_NAME/models/
LOCAL_MODEL_PATH=./models
LOCAL_MODEL_NAME=phi3-mini
LOCAL_MODEL_FILE=model.gguf
LOCAL_MAX_TOKENS=4096
LOCAL_TEMPERATURE=0.7
LOCAL_CONTEXT_SIZE=4096
LOCAL_GPU_LAYERS=0

# ============================================================================
# OpenTelemetry / Distributed Tracing Configuration
# ============================================================================
# Enable OpenTelemetry distributed tracing across all services
OTEL_ENABLED=true

# Service name (will be prefixed with service-specific names automatically)
OTEL_SERVICE_NAME=biowerk

# Exporter type: 'otlp', 'jaeger', 'console', or 'none'
# - 'otlp': Standard OpenTelemetry Protocol (production recommended)
# - 'jaeger': Jaeger-native protocol
# - 'console': Print traces to console (development/debugging)
# - 'none': Disable trace export (tracing still happens locally)
OTEL_EXPORTER_TYPE=otlp

# OTLP/Jaeger collector endpoint
# For OTLP gRPC: http://jaeger:4317
# For OTLP HTTP: http://jaeger:4318
# For Jaeger Thrift: http://jaeger:14268
OTEL_EXPORTER_ENDPOINT=http://jaeger:4317

# OTLP protocol: 'grpc' or 'http/protobuf'
OTEL_EXPORTER_PROTOCOL=grpc

# Trace sampling ratio (0.0 to 1.0)
# 1.0 = 100% of traces (development)
# 0.1 = 10% of traces (high-traffic production)
# 0.01 = 1% of traces (very high traffic)
OTEL_SAMPLING_RATIO=1.0

# Enable trace context in logs (correlate logs with traces)
OTEL_LOG_CORRELATION=true

# Span export configuration
OTEL_EXPORT_TIMEOUT=30
OTEL_MAX_QUEUE_SIZE=2048
OTEL_MAX_EXPORT_BATCH_SIZE=512

# Automatic instrumentation toggles
# Instrument database calls (PostgreSQL, MongoDB)
OTEL_INSTRUMENT_DB=true
# Instrument HTTP client calls (httpx)
OTEL_INSTRUMENT_HTTP=true
# Instrument Redis calls
OTEL_INSTRUMENT_REDIS=true

# ============================================================================
# Health Check Configuration (Kubernetes/Docker Probes)
# ============================================================================
# Enable /health (liveness) and /ready (readiness) endpoints
HEALTH_ENABLED=true

# Include PostgreSQL in health checks
HEALTH_CHECK_DB=true

# Include Redis in health checks
HEALTH_CHECK_REDIS=true

# Check external service dependencies
HEALTH_CHECK_DEPENDENCIES=true

# Health check timeout (seconds)
HEALTH_CHECK_TIMEOUT=5.0

# Startup grace period (seconds)
# Readiness checks will return "not ready" during this period
# Set based on your slowest service startup time
HEALTH_STARTUP_GRACE_PERIOD=30

# ============================================================================
# Centralized Logging Configuration (Loki)
# ============================================================================
# Log format: 'json' (recommended for Loki) or 'text' (human-readable)
# JSON format enables structured logging with automatic field extraction
LOG_FORMAT=json

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# ============================================================================
# Monitoring & Alerting Configuration
# ============================================================================
# Environment label for metrics/logs (development, staging, production)
ENVIRONMENT=development

# Region label for multi-region deployments
REGION=us-east-1

# ============================================================================
# PagerDuty Integration
# ============================================================================
# CRITICAL: Set these to your actual PagerDuty integration keys in production
# Get these from: PagerDuty → Services → Service → Integrations → Events API V2

# Primary PagerDuty service key (for critical alerts)
PAGERDUTY_SERVICE_KEY=your-pagerduty-service-key-here

# Security team PagerDuty service key (for security alerts)
PAGERDUTY_SECURITY_SERVICE_KEY=your-pagerduty-security-service-key-here

# ============================================================================
# Email Alerting Configuration (SMTP)
# ============================================================================
# SMTP server and port
# Gmail: smtp.gmail.com:587
# SendGrid: smtp.sendgrid.net:587
# AWS SES: email-smtp.us-east-1.amazonaws.com:587
SMTP_HOST=smtp.gmail.com:587

# SMTP authentication
SMTP_USERNAME=your-email@example.com
SMTP_PASSWORD=your-app-specific-password

# Alert recipient email addresses
DEFAULT_EMAIL_TO=ops-team@example.com
DATABASE_TEAM_EMAIL=database-team@example.com
COMPLIANCE_TEAM_EMAIL=compliance-team@example.com

# ============================================================================
# Slack Integration (Webhooks)
# ============================================================================
# Create incoming webhooks at: https://api.slack.com/messaging/webhooks
# Each severity level can have its own webhook and channel

# Critical alerts (24/7 on-call)
SLACK_CRITICAL_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_CRITICAL_CHANNEL=#biowerk-critical-alerts

# Warning alerts
SLACK_WARNINGS_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_WARNINGS_CHANNEL=#biowerk-warnings

# Info alerts
SLACK_INFO_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_INFO_CHANNEL=#biowerk-info

# Database alerts
SLACK_DATABASE_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_DATABASE_CHANNEL=#biowerk-database

# Security alerts
SLACK_SECURITY_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_SECURITY_CHANNEL=#biowerk-security

# Compliance alerts (GDPR)
SLACK_COMPLIANCE_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_COMPLIANCE_CHANNEL=#biowerk-compliance

# Business metrics / Product alerts
SLACK_PRODUCT_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_PRODUCT_CHANNEL=#biowerk-product

# ============================================================================
# Grafana Configuration
# ============================================================================
# Admin credentials (CHANGE IN PRODUCTION!)
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=change-this-secure-password-in-production

# Secret key for signing (generate with: openssl rand -base64 32)
GRAFANA_SECRET_KEY=change-me-to-random-32-char-string

# Database backend for Grafana (sqlite3 or postgres)
# Use PostgreSQL for production environments
GRAFANA_DB_TYPE=sqlite3
GRAFANA_DB_HOST=postgres:5432
GRAFANA_DB_NAME=grafana
GRAFANA_DB_USER=biowerk
GRAFANA_DB_PASSWORD=biowerk_dev_password

# Log level: debug, info, warn, error
GRAFANA_LOG_LEVEL=info

# ============================================================================
# Monitoring Best Practices
# ============================================================================
# For Production Deployments:
#
# 1. PagerDuty Setup:
#    - Create separate services for Critical, Security, and Database alerts
#    - Configure escalation policies (on-call rotation)
#    - Set up maintenance windows for planned downtime
#    - Test integration before going live
#
# 2. Slack Setup:
#    - Create dedicated channels for each alert severity
#    - Set appropriate channel notifications (mute info, alert on critical)
#    - Add runbook links to alert templates
#    - Configure channel retention policies
#
# 3. Email Setup:
#    - Use dedicated email distribution lists
#    - Configure SPF/DKIM for deliverability
#    - Set up email filtering rules
#    - Monitor for email delivery failures
#
# 4. Alert Tuning:
#    - Start with conservative thresholds
#    - Monitor alert volume and adjust
#    - Implement alert inhibition rules
#    - Review and update alert rules quarterly
#    - Document alert response procedures
#
# 5. Log Retention:
#    - Development: 7 days
#    - Staging: 30 days
#    - Production: 31-90 days (compliance requirements)
#    - Archive old logs to S3/GCS for long-term storage
#
# 6. Metrics Retention:
#    - High-resolution (15s): 7 days
#    - Medium-resolution (5m): 30 days
#    - Low-resolution (1h): 1 year
#    - Use remote storage (Thanos, Cortex) for long-term metrics
#
# 7. Security:
#    - Enable authentication on all monitoring endpoints
#    - Use TLS for all external communication
#    - Rotate API keys and passwords regularly
#    - Implement IP whitelisting where possible
#    - Monitor monitoring system health (meta-monitoring)
#
# 8. High Availability:
#    - Run multiple Prometheus instances with federation
#    - Use Alertmanager clustering (3+ instances)
#    - Replicate Grafana database
#    - Set up Loki in microservices mode for scale
#    - Implement backup and disaster recovery procedures
